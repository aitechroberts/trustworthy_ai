{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# C1.1 Evaluation on Object Detection\n",
        "\n",
        "## Introduction\n",
        "\n",
        "- Task: Evaluate the average precision of the data collected from SafeBench.\n",
        "\n",
        "- Submission: the plotted P-R curve, as well as the average precision at different IoU threshold level."
      ],
      "metadata": {
        "id": "dS0fvdt4EpW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "b_Kl9io2EfoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HenryLHH/24784-c1-data.git\n",
        "!cp /content/24784-c1-data/results.pkl ./\n",
        "!rm -r /content/24784-c1-data/"
      ],
      "metadata": {
        "id": "VGUoT_LRvMvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Load the Detection Results from SafeBench_v2"
      ],
      "metadata": {
        "id": "yft3dzwjEfMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = joblib.load('results.pkl')  # move the files to your current folder\n",
        "inputs.keys() # dict_keys(['image_id', 'predicted_class', 'ground_truth_bbox', 'predicted_bbox', 'conf_scores', 'num_labels'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lonMVggYRdRM",
        "outputId": "468537f6-4de4-4eda-867c-1679ea14fbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['image_id', 'predicted_class', 'ground_truth_bbox', 'predicted_bbox', 'conf_scores', 'num_labels'])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute the IoU"
      ],
      "metadata": {
        "id": "np7wJ9NsRp9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def box_area(box):\n",
        "    \"\"\" Compute the box area, given all the vertices\n",
        "    # Arguments\n",
        "        box:  (4, N) ndarray\n",
        "    # Returns\n",
        "        areas: (N, ) ndarray\n",
        "    \"\"\"\n",
        "    areas = ...   # TODO, compute the rectangle areas based on the vertices input\n",
        "    return areas\n",
        "\n",
        "def box_iou(box1, box2):\n",
        "    '''\n",
        "      Compute the iou between box1 and box2\n",
        "      ONLY consider the SINGLE ground truth, which is a simplified case\n",
        "      box1: (N, 4) ndarray\n",
        "      box2: (N, 4) ndarray\n",
        "      return: (N, ) iou scores\n",
        "    '''\n",
        "    eps = 1e-7\n",
        "\n",
        "    a1, a2 = np.split(box1, 2, axis=1)\n",
        "    b1, b2 = np.split(box2, 2, axis=1)\n",
        "\n",
        "    inter =  ...                                                          # TODO, compute the area of intersectinos\n",
        "    union = ...                                                           # TODO, compute the areas of union\n",
        "    return inter / (union + eps)\n"
      ],
      "metadata": {
        "id": "4FVpgQIWRyzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: get the box iou scores from the function you implemented above\n",
        "iou_scores = box_iou(inputs['ground_truth_bbox'], inputs['predicted_bbox'])\n",
        "\n",
        "# Build your dataframe from the dictionary\n",
        "input_dict = {\n",
        "        'image_id': inputs['image_id'],\n",
        "        'predicted_class': inputs['predicted_class'],\n",
        "        'conf_scores': inputs['conf_scores'],\n",
        "        'iou_scores': iou_scores,\n",
        "      }\n",
        "df = pd.DataFrame.from_dict(input_dict)\n",
        "df"
      ],
      "metadata": {
        "id": "PooLy9MYSYlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df.conf_scores >= 0] # drop all the frames without detection (conf scores = -1)\n",
        "\n",
        "# get all the detection results of stop signs\n",
        "df_stopsign = df.loc[(df.predicted_class=='stopsign')]\n",
        "df_stopsign"
      ],
      "metadata": {
        "id": "84DC1YNpVE1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute the Avereage Precision"
      ],
      "metadata": {
        "id": "u2gof6e_tl1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interp_ap(recall, precision, method = 'interp'):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves\n",
        "    # Arguments\n",
        "        recall:    The recall curve (list)\n",
        "        precision: The precision curve (list),\n",
        "        methods: 'continuous', 'interp'\n",
        "    # Returns\n",
        "        Average precision, precision curve, recall curve\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Append sentinel values to beginning and end\n",
        "    # Recall should start with 0.0 and end with 1.0\n",
        "    # Precision should start with 1.0 and end with 0.0\n",
        "\n",
        "    appended_recall = np.concatenate(([0.0], recall, [1.0]))\n",
        "    appended_prec_input = np.concatenate(([1.0], precision, [0.0]))\n",
        "\n",
        "    # Compute the precision envelope\n",
        "    appended_prec = np.flip(np.maximum.accumulate(np.flip(appended_prec_input)))   # get the p(r) = \\max_{r<=r'} p(r')\n",
        "\n",
        "    # Integrate area under curve\n",
        "    if method == 'interp':\n",
        "        x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
        "        ap = np.trapz(np.interp(x, appended_recall, appended_prec), x)      # get the average precision based on the areas under interpolated curves\n",
        "    else:  # 'continuous'\n",
        "        i = np.where(appended_recall[1:] != appended_recall[:-1])[0]  # points where x axis (recall) changes\n",
        "        ap = np.sum((appended_recall[i + 1] - appended_recall[i]) * appended_prec[i + 1])  # area under curve\n",
        "\n",
        "    return ap, appended_prec_input, appended_prec, appended_recall\n",
        "\n"
      ],
      "metadata": {
        "id": "qeORl8BjgZz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ap(df, num_gt, iou_thres):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves\n",
        "    # Arguments\n",
        "        df:         DataFrame inputs containing predicted classes, iou_scores, and confidence\n",
        "        num_gt:     The precision curve (list),\n",
        "        iou_thres:  IoU threshold of True Positive (TP) detection\n",
        "    # Returns\n",
        "        Average precision, precision curve, recall curve\n",
        "    \"\"\"\n",
        "    df = ...                                                           # TODO sort all the data with confidence scores 'conf_scores'\n",
        "    tp_fp = np.arange(1, len(df)+1, 1)\n",
        "    tp = ((df['iou_scores'] >= iou_thres) & (df['predicted_class'] == 'stopsign')).cumsum().values    # get the true positive sets\n",
        "    precision = tp / tp_fp      # array (N, )\n",
        "    recall = tp / num_gt        # array (N, )\n",
        "\n",
        "    ap, prec_raw, prec, recall = interp_ap(recall, precision)           # get the AP and returned curve for plot\n",
        "\n",
        "    return ap, prec_raw, prec, recall"
      ],
      "metadata": {
        "id": "IsMhUm7OUotl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "NbtBxdhHtpq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pr_curves(recall, prec_raw, prec):\n",
        "    \"\"\" Visualize the P-R curves\n",
        "        Visualize the areas under P-R curves\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(recall, prec_raw)\n",
        "    plt.plot(recall, prec)\n",
        "    plt.fill_between(recall, np.zeros_like(recall), prec, color='orange', alpha=0.2)\n",
        "    plt.legend(['Raw curve', 'Precision Envelope', 'Average Precision'])\n",
        "    plt.title('P-R Curve')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')"
      ],
      "metadata": {
        "id": "yVpGN7b-iDqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute the AP calculation"
      ],
      "metadata": {
        "id": "gH_FoxmWtsZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Traverse over IoU threshold, get AP@[0.5:0.1:0.9]\n",
        "\n",
        "for iou_thres in np.arange(0.5, 1.0, 0.1):\n",
        "    ap, prec_raw, prec, recall = compute_ap(df, inputs['num_labels'], iou_thres)\n",
        "    print('IoU Threshold: {:.2f}'.format(iou_thres), '  |    AP@{:.2f}'.format(iou_thres), ap)\n",
        "    plot_pr_curves(recall, prec_raw, prec)"
      ],
      "metadata": {
        "id": "b8kBiZLCUouR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}